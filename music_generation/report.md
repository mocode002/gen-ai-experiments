# LSTM pour la G√©n√©ration Musicale (Format ABC) üéµ

## 1. Introduction
Ce projet explore l'utilisation des **R√©seaux de Neurones R√©currents (RNN)**, et plus sp√©cifiquement des cellules **LSTM (Long Short-Term Memory)**, pour la g√©n√©ration de s√©quences cr√©atives. L'objectif est d'entra√Æner un mod√®le capable de composer de nouvelles m√©lodies de musique traditionnelle irlandaise au format **ABC notation**.

Le d√©fi principal r√©side dans la capacit√© du mod√®le √† apprendre non seulement la syntaxe des caract√®res ASCII, mais aussi la structure musicale sous-jacente (mesures, rythme, r√©p√©titions).

## 2. Dataset et Pr√©traitement
Nous utilisons le dataset **"Irishman"**, compos√© de partitions au format JSON.

### Notation ABC
La notation ABC est un format texte compact. Exemple :
```text
X: 1
T: The Title
M: 4/4
K: G
GABc dedB | ...

```

### Pipeline de Donn√©es

1. **Extraction du vocabulaire** : Identification des caract√®res uniques (notes, barres de mesure, m√©tadonn√©es).
2. **Mapping** : Cr√©ation de deux dictionnaires `char_to_idx` et `idx_to_char` pour la vectorisation.
3. **Padding** : Uniformisation de la longueur des s√©quences pour permettre l'entra√Ænement par batchs via un `DataLoader` PyTorch.

## 3. Architecture du Mod√®le

Le mod√®le est construit avec **PyTorch** et suit cette architecture :

1. **Embedding Layer** : Transforme les indices de caract√®res (discrets) en vecteurs denses continus, capturant des relations s√©mantiques entre les caract√®res.
2. **LSTM Layer** : C≈ìur du mod√®le. Contrairement aux RNN simples, le LSTM g√®re mieux les d√©pendances √† long terme (essentiel pour la structure musicale) gr√¢ce √† ses m√©canismes de *gates* (oubli, entr√©e, sortie).
3. **Dense Layer (Fully Connected)** : Projette la sortie du LSTM vers la taille du vocabulaire pour pr√©dire le caract√®re suivant.

```python
# R√©sum√© de l'architecture
self.embedding = nn.Embedding(vocab_size, embedding_dim)
self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)
self.fc = nn.Linear(hidden_size, vocab_size)
```

## 4. Entra√Ænement

* **Loss Function** : CrossEntropyLoss (probl√®me de classification multi-classes caract√®re par caract√®re).
* **Optimiseur** : Adam.
* **Techniques** : Utilisation de TensorBoard pour le logging et *Early Stopping* pour √©viter l'overfitting.

## 5. G√©n√©ration de Musique

La g√©n√©ration s'effectue caract√®re par caract√®re. √Ä chaque pas de temps, la pr√©diction est r√©inject√©e comme entr√©e pour le pas suivant.

* Nous avons explor√© l'√©chantillonnage probabiliste plut√¥t que l'approche purement "Greedy" (prendre toujours la probabilit√© max) pour introduire de la vari√©t√© et de la cr√©ativit√© dans les m√©lodies.

### Exemple de r√©sultat g√©n√©r√© :

```text
X:4
M:3/4
K:A
 A3 A A :: g | f2 ef | g2 fe | f2 ed | e3 e | f4 | d2 ef | g2 fe | f2 f2 | e2 e/^d/e/d/ | c4 | B3 z | d2 B3 | A2 z2 | F2 D2 | F2 c2 | A4 G2 | F2 F2 G2 :| A2 c2 | e3 e | 
 e2 a2 | e2 a2 | gf ed | c2 A2
```

## 6. Conclusion

Ce projet a permis de valider l'efficacit√© des LSTM pour la mod√©lisation de s√©quences complexes. Le mod√®le parvient √† respecter la syntaxe ABC et √† produire des structures musicales coh√©rentes, bien que des incoh√©rences harmoniques puissent persister sur de tr√®s longues s√©quences.